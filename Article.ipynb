{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e35509a9-d025-48dc-886d-3316ebc5e675",
   "metadata": {},
   "source": [
    "# Clothes multimodal search with Scrapegraph, Jina Clip v2 and Qdrant Vector DB üëó\n",
    "\n",
    "Hi there üëã Today we're build a small demo to search clothes from [zalando](https://zalando.com/),  directly with natural language or images. Our plan of attack is to first scrape them, embed the images using a multimodal model and then store them into a vector db so we can search!\n",
    "\n",
    "\n",
    "Scraping websites is not an easy task, most of them cannot be easily fetch with an http request and requires javascript to be loaded. If we try to make a HTTP request to zalando, we'll be blocked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b77b0f02-476b-4ad9-a3bd-95c76e3c95e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "res = requests.get(\"https://www.zalando.it/jeans-donna\")\n",
    "# we'll get 403\n",
    "res.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c8f9e0-e26b-4d74-8b16-b166d57c5a3d",
   "metadata": {},
   "source": [
    "We need something smarter, [scrapegraph](https://scrapegraphai.com/) is a perfect tool for the job. It can bypass websites blockers and allow us to define a [pydantic schema](https://docs.pydantic.dev/latest/concepts/models/) to scrape the information we want. It works by loading the website, parsing it and use LLMs to fill our schema with the data within the page.\n",
    "\n",
    "Once we get the data, we need a way to creates vector to store/search. Since we want to work with images and text, we need the heavy guns. [Jina ClipV2](https://jina.ai/news/jina-clip-v2-multilingual-multimodal-embeddings-for-text-and-images/) is a wonderful open source model that can represent as vector both images and text, thus is a perfect pick for the task.\n",
    "\n",
    "Finally, we need to save our juicy vectors somewhere. [Qdrant](https://qdrant.tech/) is my go to vector database, you can self host it with [docker](https://hub.docker.com/r/qdrant/qdrant) and it comes with an handy ui. It supports different vector quantizations technics, so we can squeeze a lot of performances!\n",
    "\n",
    "So, to recap. How plan of attack looks something like\n",
    "\n",
    "![alt](./images/flow.png)\n",
    "\n",
    "1. Scrape with Scrapegraph\n",
    "2. Embed with Jina ClipV2\n",
    "3. Store with Qdrant\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5473bcb5-00d2-4134-8f38-30ca5423ded4",
   "metadata": {},
   "source": [
    "## Setting it up\n",
    "\n",
    "We'll need a bunch of packages. I am using `uv`, so we'll stick with it. You can init your project using\n",
    "\n",
    "```\n",
    "uv init\n",
    "uv add python-dotenv scrapegraph-py==1.24.0 aiofiles sentence-transformers qdrant-client\n",
    "```\n",
    "\n",
    "Or if you prefer `pip`\n",
    "\n",
    "```\n",
    "pip install python-dotenv scrapegraph-py==1.24.0 aiofiles sentence-transformers qdrant-client\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fb1a98b8-75b9-43cd-a0b6-f6aed29c2be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m158 packages\u001b[0m \u001b[2min 0.43ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m138 packages\u001b[0m \u001b[2min 0.10ms\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!uv add python-dotenv scrapegraph-py==1.24.0 aiofiles sentence-transformers qdrant-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf92da4-c74e-481b-9c71-08c7a5be2b85",
   "metadata": {},
   "source": [
    "## Scraping\n",
    "\n",
    "First thing all, head over [scrapegraph dashboard](https://dashboard.scrapegraphai.com/) and get your api key. Create a `.env` file and put it inside\n",
    "\n",
    "```\n",
    "GAI_API_KEY=\"YOUR_API_KEY\"\n",
    "```\n",
    "\n",
    "Then we can load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0936f5d4-87c5-466a-a634-d5e9ad5d26e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "SGAI_API_KEY = os.getenv(\"SGAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4607ec75-8517-4954-addd-656aceab5030",
   "metadata": {},
   "source": [
    "Now, we need to define the data we want. Each article/item in the website looks like:\n",
    "\n",
    "\n",
    "![alt](images/zalando-article.png)\n",
    "\n",
    "We have a brand, name, description, price, image, review etc.\n",
    "\n",
    "In order to tell scrapegraph what we want to extract, we have to define a couple of pydantic schemas. Since a page contains multiple items, we'll create an `ArticleModel`, so the single article, and `ArticlesModel` containing an array of them.\n",
    "\n",
    "We can add `description` to make sure we guide the LLM into extracting the correct info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2b48a0a-1f0d-4e90-8c9c-9f233c2683d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "import asyncio\n",
    "\n",
    "\n",
    "class ArticleModel(BaseModel):\n",
    "    name: str = Field(description=\"Name of the article\")\n",
    "    brand: str = Field(description=\"Brand of the article\")\n",
    "    description: str = Field(description=\"Description of the article\")\n",
    "    price: float = Field(description=\"Price of the article\")\n",
    "    review_score: float = Field(description=\"Review score of the article, out of five.\")\n",
    "    url: str = Field(description=\"Article url\")\n",
    "    image_url: Optional[str]= Field(description=\"Article's image url\")\n",
    "\n",
    "\n",
    "class ArticlesModel(BaseModel):\n",
    "    articles: list[ArticleModel] = Field(description=\"Articles on the page, only the ones with price, review and image. Discard the others\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40eeef1-1835-472c-9699-5250d61a674d",
   "metadata": {},
   "source": [
    "Now, the fun part. We'll store our scraped data locally into a `.jsonl` file. We'll also add a `user_prompt` to guide even further scrapegraph. Since the scraping process is heavily I/O bound, we'll use their `AsyncClient` so we can fire a lot of them at once. \n",
    "\n",
    "Let's import everything and define our variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1ab676c-747c-40b8-8c19-058b5df844ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "# let' use async\n",
    "from scrapegraph_py import AsyncClient\n",
    "from scrapegraph_py.logger import sgai_logger\n",
    "sgai_logger.set_logging(level=\"INFO\")\n",
    "\n",
    "# let's use async to write to the file as well\n",
    "import aiofiles\n",
    "import json\n",
    "import asyncio\n",
    "import os\n",
    "\n",
    "JSON_PATH = \"scrape.jsonl\"\n",
    "# how much scraping request to fire at one\n",
    "BATCH_SIZE = 8\n",
    "# how many pages per category\n",
    "MAX_PAGES = 100\n",
    "\n",
    "# the user prompt to send to scrapegraph along the pydantic schemas\n",
    "\n",
    "user_prompt = \"\"\"Extract ONLY the articles in the page with price, review and image url. Discard all the others.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dae0c6d-ba43-44ae-b0f4-702068991ce9",
   "metadata": {},
   "source": [
    "To start scraping, we can use the [smartscraper](https://docs.scrapegraphai.com/services/smartscraper) method. Let's quickly see it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3466880-0c6b-4691-9b43-10c6898605cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üí¨ 2025-09-22 21:35:01,268 üîë Initializing AsyncClient\n",
      "üí¨ 2025-09-22 21:35:01,268 ‚úÖ AsyncClient initialized successfully\n",
      "üí¨ 2025-09-22 21:35:01,269 üîç Starting smartscraper request\n",
      "üí¨ 2025-09-22 21:35:01,276 üöÄ Making POST request to https://api.scrapegraphai.com/v1/smartscraper (Attempt 1/3)\n",
      "üí¨ 2025-09-22 21:35:30,913 ‚úÖ Request completed successfully: POST https://api.scrapegraphai.com/v1/smartscraper\n",
      "üí¨ 2025-09-22 21:35:30,914 ‚ú® Smartscraper request completed successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'Wrangler STRAIGHT - Jeans baggy - black',\n",
       "  'brand': 'Wrangler',\n",
       "  'description': 'No content available',\n",
       "  'price': 79.95,\n",
       "  'review_score': 0,\n",
       "  'url': 'https://www.zalando.it/wrangler-jeans-a-sigaretta-black-wr121n019-q11.html',\n",
       "  'image_url': 'https://img01.ztat.net/article/spp-media-p1/cfbd0384290c4d83a1aab8ad869e1d64/16cbb776a65e4f1eaa33a0e31ee8b66f.jpg?imwidth=300'},\n",
       " {'name': 'Stradivarius WITH POCKETS - Jeans a sigaretta - dark blue',\n",
       "  'brand': 'Stradivarius',\n",
       "  'description': 'No content available',\n",
       "  'price': 39.99,\n",
       "  'review_score': 0,\n",
       "  'url': 'https://www.zalando.it/stradivarius-jeans-baggy-dark-blue-sth21n0k1-k11.html',\n",
       "  'image_url': 'https://img01.ztat.net/article/spp-media-p1/2ec8568f05b249eab5dc984c9e93f0b3/9ad591321a5c49ac94a565c77781fa8b.jpg?imwidth=300'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining our client\n",
    "client = AsyncClient()\n",
    "# get our zalando link for women's jeans - sorry I am Italian xD\n",
    "url = \"https://www.zalando.it/jeans-donna/\"\n",
    "# get the response\n",
    "response = await client.smartscraper(\n",
    "                website_url=url,\n",
    "                user_prompt=user_prompt,\n",
    "                output_schema=ArticlesModel)\n",
    "\n",
    "response[\"result\"][\"articles\"][0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c6cda2-6419-446f-afbc-109086adbe80",
   "metadata": {},
   "source": [
    "Okay, let's make our code bulletproof. We need a function to save our data, to disk as jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f2d39735-f70b-411c-8236-9052551bfb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def save(result: dict):\n",
    "    async with aiofiles.open(JSON_PATH, 'a') as f:\n",
    "        await f.write(json.dumps(result) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d719202-e03a-4716-aed2-e4a59d0b0c35",
   "metadata": {},
   "source": [
    "Let's then call `smartscraper`, passing the `client` and the `url`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2c885b7f-984e-4224-bacd-4c7fe35d3235",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_and_save(client: AsyncClient, url: str): \n",
    "    start = perf_counter()\n",
    "    sgai_logger.info(f\"Scraping url={url}\")\n",
    "    response = await client.smartscraper(\n",
    "                website_url=url,\n",
    "                user_prompt=user_prompt,\n",
    "                output_schema=ArticlesModel)\n",
    "    await save(response)\n",
    "    sgai_logger.info(f\"Tooked {perf_counter() - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d9fed8-1c9d-456f-a71d-f0d1cd44b803",
   "metadata": {},
   "source": [
    "Finally, putting all together. We'll scrape women's jeans and t-shirt tops. We'll check first if `JSON_PATH`, if so we'll assume we had scrape already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "285cdbb5-fa49-4fb2-afbc-3fb2f1b8f28e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "async def main():\n",
    "    get_urls = [\n",
    "        lambda page: f\"https://www.zalando.it/jeans-donna/?p={page}\",\n",
    "        lambda page: f\"https://www.zalando.it/t-shirt-top-donna/?p={page}\"\n",
    "    ]\n",
    "\n",
    "    should_scrape = not os.path.exists(JSON_PATH)\n",
    "    if not should_scrape:\n",
    "        sgai_logger.info(f\"jsonl file exists, assuming we had scrape already. Quitting ...\")\n",
    "        return\n",
    "    async with AsyncClient() as client:\n",
    "        for get_url in get_urls:\n",
    "            for i in range(1, MAX_PAGES + 1, BATCH_SIZE):\n",
    "                pages = list(range(i, min(i + BATCH_SIZE, MAX_PAGES + 1)))\n",
    "                tasks = [scrape_and_save(client, get_url(page)) for page in pages]\n",
    "                await asyncio.gather(*tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b1411e2a-3a1e-41a5-97e4-6c96a935f7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üí¨ 2025-09-21 11:00:49,526 jsonl file exists, assuming we had scrape already. Quitting ...\n"
     ]
    }
   ],
   "source": [
    "# we'll take some minutes\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42036a3-1a3b-4b9c-b59d-1c39ac55ccee",
   "metadata": {},
   "source": [
    "And then you have it, each line is a page scraped!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56c510aa-b45b-4629-a282-b49775c1c7d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'PULL&BEAR BAGGY - Jeans baggy - white',\n",
       " 'brand': 'PULL&BEAR',\n",
       " 'description': 'Cropped top bianco senza maniche abbinato a pantaloni bianchi a gamba larga, con tasche frontali e chiusura a bottone. Sandali piatti marroni con borchie.',\n",
       " 'price': 35.99,\n",
       " 'review_score': 0,\n",
       " 'url': 'https://www.zalando.it/pullandbear-jeans-bootcut-white-puc21n0rs-a11.html',\n",
       " 'image_url': 'https://img01.ztat.net/article/spp-media-p1/ff33dd220e7c4827ba1b8be760e6de7c/b9ca1dcb64b04fa98b0e0d5fa38fff14.jpg?imwidth=300'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(JSON_PATH, \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        data = json.loads(line)\n",
    "        break\n",
    "\n",
    "data[\"result\"][\"articles\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9a1618-2654-4715-8ad3-c0c9f2fbe4ca",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "The heavy part is done, now we need embed each image. Recall, our `pydantic` model has an `.image_url` field that hols the link the image for an article on zalando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b0405c17-1b0a-4b23-a7b4-954de62051d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://img01.ztat.net/article/spp-media-p1/ff33dd220e7c4827ba1b8be760e6de7c/b9ca1dcb64b04fa98b0e0d5fa38fff14.jpg?imwidth=300'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"result\"][\"articles\"][0][\"image_url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b160d8e-d24f-46fb-97d5-2bec4188f1d7",
   "metadata": {},
   "source": [
    "Let's do some good programming, limiting the amount of data we have in memory each time. We'll bach process the articles, thus we can load one line of the jsonl at the time. This can be done in python with a generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f4421658-086f-4c30-afb1-afbd26e71b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_from_disk():\n",
    "    with open(JSON_PATH, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            data = json.loads(line)\n",
    "            yield data[\"result\"][\"articles\"]\n",
    "\n",
    "articles_gen = get_articles_from_disk()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a10272-b72e-4bb6-a7f4-d1dd0ef3562c",
   "metadata": {},
   "source": [
    "We'll use the wonderful [ClipV2 model made by Jina](https://jina.ai/news/jina-clip-v2-multilingual-multimodal-embeddings-for-text-and-images/) to create vectors for our images. The model has matryoshka representation; allowing (quoting from their blog post) to \"truncate the output dimensions of both text and image embeddings from 1024 down to 64, reducing storage and processing overhead while maintaining strong performance.\". We'll use 512 and use the model with [sentence_transformers](https://sbert.net/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77c0269f-7dc0-4cb5-9c9e-098c2938bc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "/Users/francescozuppichini/.cache/huggingface/modules/transformers_modules/jinaai/jina-clip-implementation/39e6a55ae971b59bea6e44675d237c99762e7ee2/modeling_clip.py:137: UserWarning: Flash attention requires CUDA, disabling\n",
      "  warnings.warn('Flash attention requires CUDA, disabling')\n",
      "/Users/francescozuppichini/.cache/huggingface/modules/transformers_modules/jinaai/jina-clip-implementation/39e6a55ae971b59bea6e44675d237c99762e7ee2/modeling_clip.py:172: UserWarning: xFormers requires CUDA, disabling\n",
      "  warnings.warn('xFormers requires CUDA, disabling')\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "EMBEDDING_SIZE = 512\n",
    "\n",
    "# initialize the model - will take some time to download it\n",
    "model = SentenceTransformer(\n",
    "    \"jinaai/jina-clip-v2\", trust_remote_code=True, truncate_dim=EMBEDDING_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff572120-4778-404f-836b-478912e0283d",
   "metadata": {},
   "source": [
    "Then, we can just pass an image url to get the embeddings, we also normalize them since we will use cosine to perform search later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03c8a7ce-5b1f-454f-95d6-e3aca82d1969",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.12898703,  0.13965721, -0.13102548,  0.09683744, -0.02695999,\n",
       "        0.04831071, -0.15391393,  0.01224686, -0.10350402,  0.05697947],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embeddings = model.encode(data[\"result\"][\"articles\"][0][\"image_url\"], normalize_embeddings=True)\n",
    "\n",
    "image_embeddings[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4416d70-041d-4de5-b76a-fb83bb1a7f1e",
   "metadata": {},
   "source": [
    "## Storing\n",
    "\n",
    "Now we need somewhere to store them. Qdrant is a perfect solution, we'll run it locally with [docker](https://docs.docker.com/engine/install/) and [docker compose](https://docs.docker.com/compose/).\n",
    "\n",
    "Assuming you have it on your system; we create a `docker-compose.yml` file\n",
    "\n",
    "```yml\n",
    "version: \"3.8\"\n",
    "\n",
    "services:\n",
    "  qdrant:\n",
    "    image: qdrant/qdrant:latest\n",
    "    ports:\n",
    "      - \"6333:6333\"\n",
    "      - \"6334:6334\"\n",
    "    volumes:\n",
    "      - ./qdrant_storage:/qdrant/storage:z\n",
    "\n",
    "```\n",
    "\n",
    "Then, simply\n",
    "\n",
    "```bash\n",
    "docker compose up -d\n",
    "```\n",
    "\n",
    "This will spin up qdrant, it also comes with a very nice ui accessible at `http://localhost:6333/dashboard#/collections` in which you can see your data.\n",
    "\n",
    "### Initialize the database\n",
    "\n",
    "We need to create a collection, we'll also use quantization to speed up things and save storage. You can read more on [qdrant doc](https://qdrant.tech/documentation/guides/quantization/) and `cosine` similarity to search. Also, we will keep the quantized vector in ram, speeding up things as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c26e6854-916a-4e27-bb2c-b1c5cedadbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x10726f230>\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "import numpy as np\n",
    "\n",
    "QDRANT_COLLECTION_NAME = \"clothes\"\n",
    "QDRANT_URL = \"http://localhost:6333\"\n",
    "\n",
    "# defining qdrant client\n",
    "client = QdrantClient(url=QDRANT_URL)\n",
    "\n",
    "# checking if we haven't created the collection already\n",
    "if not client.collection_exists(QDRANT_COLLECTION_NAME):\n",
    "    print(f\"{QDRANT_COLLECTION_NAME} created!\")\n",
    "    client.create_collection(\n",
    "        collection_name=QDRANT_COLLECTION_NAME,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=EMBEDDING_SIZE, distance=models.Distance.COSINE, on_disk=True\n",
    "        ),\n",
    "        quantization_config=models.ScalarQuantization(\n",
    "            scalar=models.ScalarQuantizationConfig(\n",
    "                type=models.ScalarType.INT8,\n",
    "                quantile=0.99,\n",
    "                always_ram=True,\n",
    "            ),\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c52c7e0-81ad-4508-aa57-02af1dfd3f03",
   "metadata": {},
   "source": [
    "We want to process our data in batches, to take advantage of the model and the network with qdrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc9fc43f-7233-4778-a5ac-1bdf1932e4e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "\n",
    "def embed_articles(data: dict) -> np.array:\n",
    "    image_urls = [el[\"image_url\"] for el in batch]\n",
    "    image_embeddings = model.encode(\n",
    "            image_urls, normalize_embeddings=True\n",
    "        )\n",
    "    return image_embeddings       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e478d9-baa7-4283-9069-4d1d602ecb86",
   "metadata": {},
   "source": [
    "### Inserting in the database\n",
    "\n",
    "Then, we can create a function to insert them into the db. We'll also store the dictionary itself, we can pass it to the `payload` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083e489e-2940-4fa4-9cbb-f469f7b97676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_articles_in_db(batch: list[dict], embeddings: np.array):\n",
    "    client.upsert(\n",
    "        collection_name=QDRANT_COLLECTION_NAME,\n",
    "        points=[\n",
    "            models.PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=vector,\n",
    "                payload=payload\n",
    "            )\n",
    "            for payload, vector in zip(batch, embeddings)\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be76e61-0e99-4a4b-87f6-ec6a8f96c8c8",
   "metadata": {},
   "source": [
    "Putting it all together, we'll check if the have points in the collection; if so we'll assume we run it already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "46507155-9e40-4559-b190-d89a04c9bcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection=clothes not empty. Exiting ...\n"
     ]
    }
   ],
   "source": [
    "def store_to_vector_db():\n",
    "    shold_insert = client.count(QDRANT_COLLECTION_NAME).count == 0\n",
    "    if not shold_insert: \n",
    "        print(f\"Collection={QDRANT_COLLECTION_NAME} not empty. Exiting ...\")\n",
    "        return\n",
    "    with tqdm(articles_gen, desc=\"Article Collections\", position=0) as pbar_collections:\n",
    "        for articles in pbar_collections:\n",
    "            batches = list(range(0, len(articles), BATCH_SIZE))\n",
    "            with tqdm(batches, desc=\"Processing Batches\", position=1, leave=False) as pbar_batches:\n",
    "                for i in pbar_batches:\n",
    "                    batch = articles[i:i + BATCH_SIZE]\n",
    "                    embeddings = embed_articles(batch)\n",
    "                    insert_articles_in_db(batch, embeddings)\n",
    "\n",
    "store_to_vector_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12e05b8-4df6-4f4b-a282-d5967cf7a884",
   "metadata": {},
   "source": [
    "We can head over the [qdrant ui](http://localhost:6333/dashboard#/collections/clothes) to see our data\n",
    "\n",
    "![alt](images/sgai-qdrant-frontend.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ec4e25-be06-42e5-8946-a12c6297b177",
   "metadata": {},
   "source": [
    "It also comes with a very cool dimension reduction tab; to explore our data even further!\n",
    "\n",
    "![alt](images/sgai-qdrant-frontend-embeddings.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf12b32-a7d7-489b-9f3e-9eb8a45e0693",
   "metadata": {},
   "source": [
    "### Searching\n",
    "\n",
    "We can now search ü•≥! With either a text query or an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "91f52e84-02ba-49f6-a017-66a02568be01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_t/kq5v2mjs6c90llk5bdqhgndc0000gn/T/ipykernel_6181/2776768032.py:8: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  res = client.search(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id='ab1f69a0-4fd0-4bce-b2ef-d01f708fc786', version=8, score=0.3037891, payload={'name': 'LeePERFECT TEE - T-shirt basic - unionall black', 'brand': 'Lee', 'description': 'T-shirt nera in cotone a maniche corte, con scollatura a girocollo e una vestibilit√† leggermente ampia, abbinata a jeans azzurri chiari con spacchi laterali.', 'price': 29.95, 'review_score': 0, 'url': 'https://www.zalando.it/lee-perfect-t-shirt-basic-unionall-black-le421d08n-q11.html', 'image_url': 'https://img01.ztat.net/article/spp-media-p1/51e721ff461a4bf5af1ea9817f8c2b58/19645bcfdfec4d8bac250cea5a4a051b.jpg?imwidth=300'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='93230ebc-05d2-430a-a5bf-13df6279f2b9', version=17, score=0.25808161, payload={'name': 'Calliope Jeans a sigaretta', 'brand': 'Calliope', 'description': '', 'price': 35.99, 'review_score': 0, 'url': 'https://www.zalando.it/calliope-jeans-a-sigaretta-nero-denim-csm21n062-q11.html', 'image_url': 'https://img01.ztat.net/article/spp-media-p1/ae1d7db5039c43b3b1aedfdbe72c5fc0/2d07663124c14a6b8c842503f3468235.jpg?imwidth=300'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='b779405a-6e45-4ac6-a2b1-65c9f338c9f0', version=4, score=0.24658957, payload={'name': 'Tommy Jeans MOM - Jeans mom fit - denim black', 'brand': 'Tommy Jeans', 'description': 'T-shirt bianca in cotone con logo rosso \"tommy jeans\", abbinata a pantaloni neri e una giacca di pelle nera. Sneakers bianche completano il look.', 'price': 64.99, 'review_score': 0, 'url': 'https://www.zalando.it/tommy-jeans-mom-jeans-baggy-denim-black-tob21n0tq-q11.html', 'image_url': 'https://img01.ztat.net/article/spp-media-p1/91c74d136e8d470cbebf8f9d6afb7b5b/db59f4efc20347919e0e1a2148aeb986.jpg?imwidth=300'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='e66cab2d-e0fa-4ea1-91e5-e69e6b4b7586', version=10, score=0.24387194, payload={'name': 'Stradivarius WITH RHINESTONES - Jeans baggy - mottled light grey', 'brand': 'Stradivarius', 'description': 'No content available', 'price': 49.99, 'review_score': 0, 'url': 'https://www.zalando.it/stradivarius-jeans-a-sigaretta-mottled-light-grey-sth21n0om-c11.html', 'image_url': 'https://img01.ztat.net/article/spp-media-p1/249d670066a34cbf9f2d6e3d5a782cf2/8b8e300ac87747c7b92aac6d81ff7a96.jpg?imwidth=300'}, vector=None, shard_key=None, order_value=None)]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 't-shirt black'\n",
    "# call the model to embed the query\n",
    "query_embeddings = model.encode(\n",
    "    query, prompt_name='retrieval.query', normalize_embeddings=True\n",
    "    \n",
    ")  \n",
    "# getting results\n",
    "res = client.search(\n",
    "        collection_name=QDRANT_COLLECTION_NAME,\n",
    "        query_vector=query_embeddings.tolist(),\n",
    "        limit=4,\n",
    "    )\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a007bfe0-b97f-4074-8b68-1a8582a84090",
   "metadata": {},
   "source": [
    "Let's define a function to show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ad8b53e-a0cc-4572-9eca-95130225dfec",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bytes' object has no attribute 'payload'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m     html += \u001b[33m\"\u001b[39m\u001b[33m</div>\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m     display(HTML(html))\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mshow_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mshow_images\u001b[39m\u001b[34m(res)\u001b[39m\n\u001b[32m      4\u001b[39m html = \u001b[33m\"\u001b[39m\u001b[33m<div style=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mdisplay:grid;grid-template-columns:repeat(4,1fr);gap:10px;\u001b[39m\u001b[33m'\u001b[39m\u001b[33m>\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m res:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     html += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m<img src=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpayload\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mimage_url\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m style=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mwidth:200px;height:auto;object-fit:cover;border:1px solid #ddd;\u001b[39m\u001b[33m'\u001b[39m\u001b[33m>\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m html += \u001b[33m\"\u001b[39m\u001b[33m</div>\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m display(HTML(html))\n",
      "\u001b[31mAttributeError\u001b[39m: 'bytes' object has no attribute 'payload'"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_images(res):\n",
    "    html = \"<div style='display:grid;grid-template-columns:repeat(4,1fr);gap:10px;'>\"\n",
    "    for result in res:\n",
    "        html += f\"<img src='{result.payload['image_url']}' style='width:200px;height:auto;object-fit:cover;border:1px solid #ddd;'>\"\n",
    "    html += \"</div>\"\n",
    "    display(HTML(html))\n",
    "\n",
    "show_images(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3e8a0cc5-532d-46a8-a146-1d09477cf0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_t/kq5v2mjs6c90llk5bdqhgndc0000gn/T/ipykernel_6181/1374654977.py:9: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  res = client.search(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://d1fufvy4xao6k9.cloudfront.net/images/landings/43/shirts-mob-1.jpg\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or using either a pil image or a url\n",
    "from IPython.display import Image, display\n",
    "image_url = \"https://d1fufvy4xao6k9.cloudfront.net/images/landings/43/shirts-mob-1.jpg\"\n",
    "# call the model to embed the query\n",
    "query_embeddings = model.encode(\n",
    "            image_url, normalize_embeddings=True\n",
    "        )\n",
    "# getting results\n",
    "res = client.search(\n",
    "        collection_name=QDRANT_COLLECTION_NAME,\n",
    "        query_vector=query_embeddings.tolist(),\n",
    "        limit=4,\n",
    "    )\n",
    "\n",
    "Image(url=image_url, width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f6f09ecc-1060-4236-9207-e3f57b4c3477",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display:grid;grid-template-columns:repeat(4,1fr);gap:10px;'><img src='https://img01.ztat.net/article/spp-media-p1/13f14ab6cacc4f33aea6cfadb0fac207/7aef5c18accf478d860a3331103b73f6.jpg?imwidth=300' style='width:100%;height:200px;object-fit:cover;border:1px solid #ddd;'><img src='https://img01.ztat.net/article/spp-media-p1/7677dca00ac64142bbd7f40a123fa9f1/5e505f360e094ad89fe394ac376261a8.jpg?imwidth=300' style='width:100%;height:200px;object-fit:cover;border:1px solid #ddd;'><img src='https://img01.ztat.net/article/spp-media-p1/d1d3b10d972747edb8f0820108654c09/c48761ba8f78492cb1e44d629c0532b9.jpg?imwidth=300' style='width:100%;height:200px;object-fit:cover;border:1px solid #ddd;'><img src='https://img01.ztat.net/article/spp-media-p1/62cfd75b7e634ec2809e0ab7f808adce/5b6ee869474b4268a5c7982548c69e2e.jpg' style='width:100%;height:200px;object-fit:cover;border:1px solid #ddd;'></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56105822-4300-4870-a244-211ce6f2c518",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "So we show how to scrape, embed and search using both text and images items from zalando. We could expand it further by actually scraping more data and implement a re-ranker to show up results that are really relevant to the query - but I'll leave it to you here :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
